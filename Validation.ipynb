{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from notifyme import notify\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.uint16'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(\"./train.csv\")\n",
    "\n",
    "grapheme_root_ohe = OneHotEncoder(dtype=np.uint16,sparse=False)\n",
    "vowel_diacritic_ohe = OneHotEncoder(dtype=np.uint16,sparse=False)\n",
    "consonant_diacritic_ohe = OneHotEncoder(dtype=np.uint16,sparse=False)\n",
    "\n",
    "grapheme_root_ohe.fit(labels[['grapheme_root']])\n",
    "vowel_diacritic_ohe.fit(labels[['vowel_diacritic']])\n",
    "consonant_diacritic_ohe.fit(labels[['consonant_diacritic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (64, 64, 1),name=\"inputs\")\n",
    "model = Conv2D(filters=32, kernel_size=(4, 4), padding='SAME', activation='relu', input_shape=(64, 64, 1))(inputs)\n",
    "model = Conv2D(filters=32, kernel_size=(4, 4), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=64, kernel_size=(4, 4), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=128, kernel_size=(4, 4), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=256, kernel_size=(6, 6), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Flatten()(model)\n",
    "model = Dense(1024, activation = \"relu\")(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "dense = Dense(512, activation = \"relu\")(model)\n",
    "\n",
    "head_root = Dense(168, activation = 'softmax',name=\"grapheme_root\")(dense)\n",
    "head_vowel = Dense(11, activation = 'softmax',name='vowel_diacritic')(dense)\n",
    "head_consonant = Dense(7, activation = 'softmax',name='consonant_diacritic')(dense)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(\"./.keras_checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe271cfc198>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(img,pad=True):\n",
    "    W_THRESH = 8\n",
    "    H_THRESH = 8\n",
    "    PAD = 3 if pad else 0\n",
    "\n",
    "    W_MIN,W_MAX = np.where(img.std(axis=0) > W_THRESH)[0][[0,-1]]\n",
    "    H_MIN,H_MAX = np.where(img.std(axis=1) > H_THRESH)[0][[0,-1]]\n",
    "    \n",
    "    return np.pad(img[H_MIN:H_MAX,W_MIN:W_MAX],PAD,constant_values=253)\n",
    "\n",
    "def resize(img):\n",
    "    img = crop(img.reshape(137,236).astype(np.uint8))\n",
    "    ret,img = cv.threshold(img,110,255,cv.THRESH_BINARY_INV)    \n",
    "    return cv.resize(img,(64,64)).astype(np.uint8).reshape(64,64,1)\n",
    "\n",
    "def input_flow(x,y,batch_size=200):\n",
    "    for i in range(batch_size,x.shape[0],batch_size):\n",
    "        rows = x.iloc[i-batch_size:i].values\n",
    "        yield (\n",
    "                {\"inputs\":np.apply_along_axis(resize,axis=1,arr=rows)},\n",
    "                {\n",
    "                    \"grapheme_root\":y[0][i-batch_size:i],\n",
    "                    'vowel_diacritic':y[1][i-batch_size:i],\n",
    "                    'consonant_diacritic':y[2][i-batch_size:i]\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(file_id):\n",
    "    df = pd.merge(\n",
    "            pd.read_parquet(f\"./train_image_data_{file_id}.parquet\"),\n",
    "            labels,\n",
    "            on='image_id'\n",
    "        )\n",
    "    \n",
    "    grapheme_root = grapheme_root_ohe.transform(df.grapheme_root.reshape(-1,1))\n",
    "    vowel_diacritic = vowel_diacritic_ohe.transform(df.vowel_diacritic.reshape(-1,1))\n",
    "    consonant_diacritic = consonant_diacritic_ohe.transform(df.consonant_diacritic.reshape(-1,1))\n",
    "    \n",
    "    df = df.drop(columns=['image_id','grapheme_root','vowel_diacritic','consonant_diacritic','grapheme'])\n",
    "    \n",
    "    return df,(grapheme_root,vowel_diacritic,consonant_diacritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = get_train_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2906313224647387,\n",
       " 0.19009864,\n",
       " 0.053675424,\n",
       " 0.04685738,\n",
       " 0.9434462,\n",
       " 0.98428285,\n",
       " 0.985737]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "gen = input_flow(X,Y,batch_size=BATCH_SIZE)\n",
    "\n",
    "model.evaluate_generator(gen,steps=X.shape[0]//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
